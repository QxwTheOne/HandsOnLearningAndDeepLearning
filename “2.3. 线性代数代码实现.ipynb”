{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"“2.3. 线性代数代码实现.ipynb”","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_preliminaries/linear-algebra.ipynb","timestamp":1630757571805}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"Ll3JVUw0uXTE"},"source":["# 线性代数\n","\n","\n","## 标量\n","\n","(**标量由只有一个元素的张量表示**)。在下面的代码中，我们实例化两个标量，并使用它们执行一些熟悉的算术运算，即加法、乘法、除法和指数。\n"]},{"cell_type":"code","metadata":{"origin_pos":2,"tab":["pytorch"],"id":"RXr5Ze7RuXTI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630757977849,"user_tz":-480,"elapsed":4166,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"dde13b58-5138-4b8d-def8-96736379ed6d"},"source":["import torch\n","\n","x = torch.tensor([3.0])\n","y = torch.tensor([2.0])\n","\n","x + y, x * y, x / y, x**y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"g5KQPi4EuXTK"},"source":["## 向量\n","\n","[**你可以将向量视为标量值组成的列表**]。我们将这些标量值称为向量的*元素*（element）或*分量*（component）。\n","我们通过一维张量处理向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。\n"]},{"cell_type":"code","metadata":{"origin_pos":6,"tab":["pytorch"],"id":"d3pPC-O7uXTK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758296757,"user_tz":-480,"elapsed":540,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"d94cda7a-530c-4dd9-c945-c0b7a5df9ca1"},"source":["x = torch.arange(4)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"JLNwjyxkuXTL"},"source":["我们可以使用下标来引用向量的任一元素。例如，我们可以通过$x_i$来引用第$i$个元素。注意，元素$x_i$是一个标量，所以我们在引用它时不会加粗。大量文献认为列向量是向量的默认方向，在本书中也是如此。在数学中，向量$\\mathbf{x}$可以写为：\n","\n","$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n","\n","\n","其中$x_1,\\ldots,x_n$是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)。\n"]},{"cell_type":"code","metadata":{"origin_pos":10,"tab":["pytorch"],"id":"62tPrPlruXTL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758317610,"user_tz":-480,"elapsed":570,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"6cf5fc16-ca1a-45e7-a2c0-f217886b486a"},"source":["x[3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"adLL1zdzuXTM"},"source":["### 长度、维度和形状\n","\n","向量只是一个数字数组。就像每个数组都有一个长度一样，每个向量也是如此。在数学表示法中，如果我们想说一个向量$\\mathbf{x}$由$n$个实值标量组成，我们可以将其表示为$\\mathbf{x}\\in\\mathbb{R}^n$。向量的长度通常称为向量的*维度*（dimension）。\n","\n","与普通的Python数组一样，我们可以通过调用Python的内置`len()`函数来[**访问张量的长度**]。\n"]},{"cell_type":"code","metadata":{"origin_pos":14,"tab":["pytorch"],"id":"ikGAcrRluXTM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758371373,"user_tz":-480,"elapsed":545,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"617adcb8-ebfb-4e57-fefb-c937c21b65b2"},"source":["len(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"Z9mLEDWQuXTN"},"source":["当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。形状（shape）是一个元组，列出了张量沿每个轴的长度（维数）。对于(**只有一个轴的张量，形状只有一个元素。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":18,"tab":["pytorch"],"id":"ZZKj4RIpuXTN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758385493,"user_tz":-480,"elapsed":543,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"90a8579e-9863-47d6-e7a5-8222ea0dca8d"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"w3h4nvTTuXTO"},"source":["请注意，*维度*（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。为了清楚起见，我们在此明确一下。*向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。然而，张量的维度用来表示张量具有的轴数。在这个意义上，张量的某个轴的维数就是这个轴的长度。\n","\n","## 矩阵\n","\n","正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。矩阵，我们通常用粗体、大写字母来表示（例如，$\\mathbf{X}$、$\\mathbf{Y}$和$\\mathbf{Z}$），在代码中表示为具有两个轴的张量。\n","\n","在数学表示法中，我们使用$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$来表示矩阵$\\mathbf{A}$，其由$m$行和$n$列的实值标量组成。直观地，我们可以将任意矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$视为一个表格，其中每个元素$a_{ij}$属于第$i$行第$j$列：\n","\n","$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n","\n","\n","对于任意$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$,$\\mathbf{A}$的形状是（$m$,$n$）或$m \\times n$。当矩阵具有相同数量的行和列时，其形状将变为正方形；因此，它被称为*方矩阵*（square matrix）。\n","\n","当调用函数来实例化张量时，我们可以[**通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的矩阵**]。\n"]},{"cell_type":"code","metadata":{"origin_pos":22,"tab":["pytorch"],"id":"o_aHFZE9uXTO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758530702,"user_tz":-480,"elapsed":539,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"cd049d42-e3f3-484c-e6d4-ea38ba1aacfe"},"source":["A = torch.arange(20).reshape(5, 4)\n","A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"-YbhLBvSuXTO"},"source":["\n","\n","现在我们在代码中访问(**矩阵的转置**)。\n"]},{"cell_type":"code","metadata":{"origin_pos":26,"tab":["pytorch"],"id":"0XxULRkhuXTP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758789032,"user_tz":-480,"elapsed":800,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"25d9a3ea-6b21-43a4-8839-88894c8c9150"},"source":["A.T"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  4,  8, 12, 16],\n","        [ 1,  5,  9, 13, 17],\n","        [ 2,  6, 10, 14, 18],\n","        [ 3,  7, 11, 15, 19]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"HgVDTeZtuXTP"},"source":["作为方矩阵的一种特殊类型，[***对称矩阵*（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$**]。这里我们定义一个对称矩阵$\\mathbf{B}$：\n"]},{"cell_type":"code","metadata":{"origin_pos":30,"tab":["pytorch"],"id":"6VocVxFduXTQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758795839,"user_tz":-480,"elapsed":559,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"0ad5b492-ba46-4f7a-c96c-f3180e1c7a33"},"source":["B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n","B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [2, 0, 4],\n","        [3, 4, 5]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"MCmJT-QiuXTQ"},"source":["现在我们将`B`与它的转置进行比较。\n"]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"id":"kTlGAqBkuXTQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758799431,"user_tz":-480,"elapsed":7,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"7bb883d6-21e7-47bc-eab1-c0e8b008703c"},"source":["B == B.T"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True, True, True],\n","        [True, True, True],\n","        [True, True, True]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"076r2n4AuXTQ"},"source":["\n","## 张量\n","\n","[**就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构**]。"]},{"cell_type":"code","metadata":{"origin_pos":38,"tab":["pytorch"],"id":"acDt1Ed3uXTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758926156,"user_tz":-480,"elapsed":829,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"9d525b09-21be-4573-86c3-bb0f267587aa"},"source":["X = torch.arange(24).reshape(2, 3, 4)\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"Xzwe_u4UuXTR"},"source":["## 张量算法的基本性质\n","\n","[**给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量**]。例如，将两个相同形状的矩阵相加会在这两个矩阵上执行元素加法。\n"]},{"cell_type":"code","metadata":{"origin_pos":42,"tab":["pytorch"],"id":"HkrgpsPPuXTR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630758987428,"user_tz":-480,"elapsed":6,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"8cc4ad0c-b976-4d33-9c89-f5b9dd0bb65d"},"source":["A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n","A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RExKsSLlz1K_","executionInfo":{"status":"ok","timestamp":1630758987429,"user_tz":-480,"elapsed":4,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"72082b1b-a720-41fc-d64b-335fda1464f7"},"source":["A + B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  2.,  4.,  6.],\n","        [ 8., 10., 12., 14.],\n","        [16., 18., 20., 22.],\n","        [24., 26., 28., 30.],\n","        [32., 34., 36., 38.]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"xVHIn1mnuXTR"},"source":["具体而言，[**两个矩阵的按元素乘法称为*哈达玛积*（Hadamard product）（数学符号$\\odot$）**]。对于矩阵$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，其中第$i$行和第$j$列的元素是$b_{ij}$。矩阵$\\mathbf{A}$哈达玛积为：\n","\n","$$\n","\\mathbf{A} \\odot \\mathbf{B} =\n","\\begin{bmatrix}\n","    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n","    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n","    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n","\\end{bmatrix}.\n","$$\n"]},{"cell_type":"code","metadata":{"origin_pos":46,"tab":["pytorch"],"id":"R9CIV7l0uXTS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630759005321,"user_tz":-480,"elapsed":539,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"4f7ab73a-aaf3-421c-f588-95281c27c5bb"},"source":["A * B"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0.,   1.,   4.,   9.],\n","        [ 16.,  25.,  36.,  49.],\n","        [ 64.,  81., 100., 121.],\n","        [144., 169., 196., 225.],\n","        [256., 289., 324., 361.]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"origin_pos":48,"id":"sp78lXtDuXTS"},"source":["将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。类似广播机制\n"]},{"cell_type":"code","metadata":{"origin_pos":50,"tab":["pytorch"],"id":"L62qKCHGuXTS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630759059579,"user_tz":-480,"elapsed":534,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"9852a8cf-9e7e-483f-b5f5-f6204c5c8778"},"source":["a = 2\n","X = torch.arange(24).reshape(2, 3, 4)\n","a + X, (a * X).shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[ 2,  3,  4,  5],\n","          [ 6,  7,  8,  9],\n","          [10, 11, 12, 13]],\n"," \n","         [[14, 15, 16, 17],\n","          [18, 19, 20, 21],\n","          [22, 23, 24, 25]]]), torch.Size([2, 3, 4]))"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"origin_pos":52,"id":"sFyKofMPuXTS"},"source":["## 降维\n","\n","\n","[**计算其元素的和**]。\n"]},{"cell_type":"code","metadata":{"origin_pos":54,"tab":["pytorch"],"id":"DMSZOlGiuXTS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760313871,"user_tz":-480,"elapsed":1414,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"fd7e755d-7fb0-4fec-87a4-25ef260f99ef"},"source":["Mx = torch.arange(4, dtype=torch.float32)\n","x, x.sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 1, 2, 3, 4]), tensor(10))"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"origin_pos":58,"tab":["pytorch"],"id":"Yo5zeuZUuXTT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760318858,"user_tz":-480,"elapsed":1384,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"a6153bb2-01cd-4fd8-ed05-0ec9f12019cf"},"source":["A.shape, A.sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 4]), tensor(190.))"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"origin_pos":60,"id":"MR4tNDDpuXTT"},"source":["[**指定张量沿哪一个轴来通过求和降低维度**]。以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定`axis=0`。\n","由于输入矩阵沿0轴降维以生成输出向量，因此输入的轴0的维数在输出形状中丢失。\n","\n","二维[5,4]=>[4] 若axis = 0;\n","三维[2,3,4]=>[2,3] 若aixs = 2\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmqHzN_V5Ied","executionInfo":{"status":"ok","timestamp":1630760378193,"user_tz":-480,"elapsed":11,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"179482b8-4bdc-4828-ca7e-91a15846f211"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"origin_pos":62,"tab":["pytorch"],"id":"WcsBW2dNuXTT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760424738,"user_tz":-480,"elapsed":882,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"02c9c0fc-9dc0-4894-ec0f-fb454113132c"},"source":["A_sum_axis0 = A.sum(axis=0)\n","A_sum_axis0, A_sum_axis0.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([40., 45., 50., 55.]), torch.Size([4]))"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"origin_pos":64,"id":"54kdSHnjuXTT"},"source":["指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入的轴1的维数在输出形状中消失。\n"]},{"cell_type":"code","metadata":{"origin_pos":66,"tab":["pytorch"],"id":"CXl32joGuXTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760423146,"user_tz":-480,"elapsed":834,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"f25d690f-0171-4851-e98c-008152a02072"},"source":["A_sum_axis1 = A.sum(axis=1)\n","A_sum_axis1, A_sum_axis1.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"origin_pos":68,"id":"Syif0ltHuXTU"},"source":["沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。\n"]},{"cell_type":"code","metadata":{"origin_pos":70,"tab":["pytorch"],"id":"Tgl5N2hZuXTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760516513,"user_tz":-480,"elapsed":540,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"ae249944-74f3-4f07-eb8e-6fb8c8f1c744"},"source":["A.sum(axis=[0, 1])  # Same as `A.sum()`"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(190.)"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"origin_pos":72,"id":"8hf4nk3DuXTU"},"source":["[**一个与求和相关的量是*平均值*（mean或average）**]。我们通过将总和除以元素总数来计算平均值。在代码中，我们可以调用函数来计算任意形状张量的平均值。\n"]},{"cell_type":"code","metadata":{"origin_pos":74,"tab":["pytorch"],"id":"Jqhndb60uXTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760574516,"user_tz":-480,"elapsed":685,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"65a45f79-70c8-47be-de52-ed3878964a4f"},"source":["# 平均值 求和     数组元素个数\n","A.mean(), A.sum() / A.numel()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(9.5000), tensor(9.5000))"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"origin_pos":76,"id":"f7BU5HKnuXTV"},"source":["同样，计算平均值的函数也可以沿指定轴降低张量的维度。\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajJ1nKKV6GM1","executionInfo":{"status":"ok","timestamp":1630760628607,"user_tz":-480,"elapsed":9,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"08b3ae9c-eb1d-45d6-9e10-9ea8c1c6cdd0"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"origin_pos":78,"tab":["pytorch"],"id":"nUoR3lmwuXTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630760618856,"user_tz":-480,"elapsed":705,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"07c2d243-bd06-4c28-bfa3-d75919557bea"},"source":["#                  0维个数，也就是有多少行\n","A.mean(axis=0), A.sum(axis=0) / A.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"origin_pos":80,"id":"mjikzv1buXTV"},"source":["### 非降维求和\n","\n","保证二维不会降到一维\n"]},{"cell_type":"code","metadata":{"origin_pos":82,"tab":["pytorch"],"id":"oLnxa4m3uXTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630761503371,"user_tz":-480,"elapsed":731,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"37e957b4-50d3-46c5-bea3-efe0a951d090"},"source":["sum_A = A.sum(axis=1, keepdims=True)\n","sum_A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 6.],\n","        [22.],\n","        [38.],\n","        [54.],\n","        [70.]])"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"origin_pos":84,"id":"W99W_ArguXTV"},"source":["例如，由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以(**通过广播将`A`除以`sum_A`**)。\n","\n","疑惑，怎么实现的？？？？\n"]},{"cell_type":"code","metadata":{"origin_pos":86,"tab":["pytorch"],"id":"m-jJPbVmuXTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630761505360,"user_tz":-480,"elapsed":646,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"3427f0be-b7f3-4eec-ad81-8755cedab7e5"},"source":["A / sum_A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n","        [0.1818, 0.2273, 0.2727, 0.3182],\n","        [0.2105, 0.2368, 0.2632, 0.2895],\n","        [0.2222, 0.2407, 0.2593, 0.2778],\n","        [0.2286, 0.2429, 0.2571, 0.2714]])"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"origin_pos":88,"id":"F4r90ZjsuXTW"},"source":["如果我们想沿[**某个轴计算`A`元素的累积总和**]，比如`axis=0`（按行计算），我们可以调用`cumsum`函数。此函数不会沿任何轴降低输入张量的维度。\n","\n","按行，从上往下，以此累计求和。\n","\n","比如：第一列中，0+4 = 4(第二行)4+8=12（第三行）;12+12=24（第四行）；24+16=40第五行；\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr5CD5zl9lYW","executionInfo":{"status":"ok","timestamp":1630761732251,"user_tz":-480,"elapsed":571,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"cf3719f9-7f70-475a-f748-e209d9e2c1b4"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","metadata":{"origin_pos":90,"tab":["pytorch"],"id":"hgmw_9uRuXTW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630761733031,"user_tz":-480,"elapsed":12,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"d4fd84b9-7fcf-488b-992b-90d57ff4e130"},"source":["A.cumsum(axis=0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  6.,  8., 10.],\n","        [12., 15., 18., 21.],\n","        [24., 28., 32., 36.],\n","        [40., 45., 50., 55.]])"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"origin_pos":92,"id":"O-xIJv2XuXTW"},"source":["## 点积（Dot Product）\n","点积。给定两个向量$\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$，它们的*点积*（dotproduct）$\\mathbf{x}^\\top\\mathbf{y}$（或$\\langle\\mathbf{x},\\mathbf{y}\\rangle$）是相同位置的按元素乘积的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。\n","\n","[~~点积是相同位置的按元素乘积的和~~]\n"]},{"cell_type":"code","metadata":{"origin_pos":94,"tab":["pytorch"],"id":"J4o10wGpuXTW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630761900826,"user_tz":-480,"elapsed":514,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"b96213c3-8277-40ef-dce6-6dca3f192730"},"source":["x_1 = torch.arange(5, dtype = torch.float32)\n","y = torch.ones(5, dtype = torch.float32)\n","x_1, y, torch.dot(x_1, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 1., 2., 3., 4.]), tensor([1., 1., 1., 1., 1.]), tensor(10.))"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","metadata":{"origin_pos":96,"id":"PTPWQZGQuXTW"},"source":["注意，(**我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积**)：\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j349VnZD_LQ2","executionInfo":{"status":"ok","timestamp":1630761962511,"user_tz":-480,"elapsed":3,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"cfccbb08-20e1-4dc8-c3a4-4b603a2f52ab"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woTC--r__LJP","executionInfo":{"status":"ok","timestamp":1630761964446,"user_tz":-480,"elapsed":2,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"ab70b4c9-7600-4d82-9f0e-f9fca60d54fe"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 1., 1., 1., 1.])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2r8v85N9_HO5","executionInfo":{"status":"ok","timestamp":1630761943275,"user_tz":-480,"elapsed":8,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"ae08f201-e179-4423-b247-ad6c9dbf9403"},"source":["x * y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 2., 3., 4.])"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"origin_pos":98,"tab":["pytorch"],"id":"UegHUZVguXTW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630761915628,"user_tz":-480,"elapsed":749,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"72f7a827-f4be-4d3c-ec3c-cffe0a118be5"},"source":["torch.sum(x * y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(10.)"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","metadata":{"origin_pos":100,"id":"E_r5tinduXTW"},"source":["\n","## 矩阵-向量积\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCD38jfz_l5N","executionInfo":{"status":"ok","timestamp":1630762074472,"user_tz":-480,"elapsed":1414,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"e2cf9901-f776-4024-a3c5-b51be6f811e3"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"082zENZNAG03"},"source":["torch.mv()是矩阵和向量相乘，类似于torch.mm().等价于Ax^T\n"]},{"cell_type":"code","metadata":{"id":"_Zx02VDt_nov"},"source":["x = torch.arange(4, dtype = torch.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCXuZhZCAF7V","executionInfo":{"status":"ok","timestamp":1630762215153,"user_tz":-480,"elapsed":1718,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"3bde8fc8-fddd-4a89-928b-bfa38fbadec9"},"source":["A.shape, x.shape, torch.mv(A, x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"origin_pos":104,"id":"-xbQjwLRuXTX"},"source":["## 矩阵-矩阵乘法\n","**矩阵-矩阵乘法**（matrix-matrix multiplication）应该很简单。\n","\n","点积$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n","\n","$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\\\\n","\\mathbf{a}^\\top_{2} \\\\\n","\\vdots \\\\\n","\\mathbf{a}^\\top_n \\\\\n","\\end{bmatrix}\n","\\begin{bmatrix}\n"," \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n","\\end{bmatrix}\n","= \\begin{bmatrix}\n","\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n"," \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n"," \\vdots & \\vdots & \\ddots &\\vdots\\\\\n","\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n","\\end{bmatrix}.\n","$$\n","\n","[**我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作是简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \\times m$矩阵**]。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFxsZQJZAg5D","executionInfo":{"status":"ok","timestamp":1630762313757,"user_tz":-480,"elapsed":586,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"ff3086c3-aa31-4069-bbc4-df800b10c730"},"source":["A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.],\n","        [12., 13., 14., 15.],\n","        [16., 17., 18., 19.]])"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","metadata":{"origin_pos":106,"tab":["pytorch"],"id":"N9Bn6A7PuXTX","outputId":"7ddbc33e-9e8f-4e92-abce-944f2142ed35"},"source":["B = torch.ones(4, 3)\n","torch.mm(A, B)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([[ 6.,  6.,  6.],\n","        [22., 22., 22.],\n","        [38., 38., 38.],\n","        [54., 54., 54.],\n","        [70., 70., 70.]])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"origin_pos":108,"id":"xm2dabYzuXTX"},"source":["矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与\"哈达玛积\"混淆。\n","\n","## 范数\n","$L_2$范数。假设$n$维向量$\\mathbf{x}$中的元素是$x_1,\\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]\n","\n","(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n","\n","其中，在$L_2$范数中常常省略下标$2$，也就是说，$\\|\\mathbf{x}\\|$等同于$\\|\\mathbf{x}\\|_2$。在代码中，我们可以按如下方式计算向量的$L_2$范数。\n"]},{"cell_type":"code","metadata":{"origin_pos":110,"tab":["pytorch"],"id":"nxshnp5CuXTY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630762373241,"user_tz":-480,"elapsed":869,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"5aa6e33d-62e7-4443-8e34-4e923af99cc1"},"source":["u = torch.tensor([3.0, -4.0])\n","torch.norm(u)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","metadata":{"origin_pos":112,"id":"h48TWoy3uXTY"},"source":["在深度学习中，我们更经常地使用$L_2$范数的平方。你还会经常遇到[**$L_1$范数，它表示为向量元素的绝对值之和：**]\n","\n","(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n","\n","与$L_2$范数相比，$L_1$范数受异常值的影响较小。为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。\n"]},{"cell_type":"code","metadata":{"origin_pos":114,"tab":["pytorch"],"id":"OQpdjFjzuXTY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630762378607,"user_tz":-480,"elapsed":1036,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"c9aa2042-e894-4368-99a9-93c8ee884bdb"},"source":["torch.abs(u).sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7.)"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","metadata":{"origin_pos":116,"id":"S8OILlJsuXTY"},"source":["$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：\n","\n","$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n","\n","类似于向量的$L_2$范数，[**矩阵**]$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$(**的*弗罗贝尼乌斯范数*（Frobenius norm）是矩阵元素平方和的平方根：**)\n","\n","(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n","\n","弗罗贝尼乌斯范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。\n","调用以下函数将计算矩阵的弗罗贝尼乌斯范数。\n"]},{"cell_type":"code","metadata":{"origin_pos":118,"tab":["pytorch"],"id":"cKQomjQbuXTY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630762389723,"user_tz":-480,"elapsed":593,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"97285b3f-d11e-449c-fc62-fc312c0ea8d0"},"source":["torch.norm(torch.ones((4, 9)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(6.)"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","metadata":{"origin_pos":120,"id":"QoxcdNszuXTZ"},"source":["\n","\n","## 练习\n","\n","1. 证明一个矩阵$\\mathbf{A}$的转置的转置是$\\mathbf{A}$：$(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。\n","1. 给出两个矩阵$\\mathbf{A}$和$\\mathbf{B}$，显示转置的和等于和的转置：$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$。\n","1. 给定任意方矩阵$\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$总是对称的吗?为什么?\n","1.我们在本节中定义了形状（2,3,4）的张量`X`。`len(X)`的输出结果是什么？\n","1.对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?\n","1.运行`A/A.sum(axis=1)`，看看会发生什么。你能分析原因吗？\n","1.当你在曼哈顿的两点之间旅行时，你需要在坐标上走多远，也就是说，就大街和街道而言？你能斜着走吗？\n","1.考虑一个具有形状（2,3,4）的张量，在轴0,1,2上的求和输出是什么形状?\n","1.向`linalg.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qg1uMJC6BRfX","executionInfo":{"status":"ok","timestamp":1630762600142,"user_tz":-480,"elapsed":736,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"8431992f-1dd6-446c-b62f-e527d1b274da"},"source":["x = torch.zeros(2,3,4)\n","len(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9QXE1DGABsHx","executionInfo":{"status":"ok","timestamp":1630762678584,"user_tz":-480,"elapsed":1673,"user":{"displayName":"xuewen Qin","photoUrl":"","userId":"02728921575002775664"}},"outputId":"7d84cab1-68c0-442c-97a9-1fae3d935c0b"},"source":["#1\n","A=torch.arange(21,dtype=torch.float32).reshape(3,7)\n","print(A==A.T.T)\n","\n","#2\n","B=torch.ones_like(A)\n","print((A.T+B.T)==(A+B).T)\n","\n","#3\n","C=torch.randn(4,4)\n","print((C+C.T).T==(C+C.T))\n","\n","#4、5  len(D)是显示D在0轴上的长度！\n","D=torch.randn(6,3,4)\n","print(len(D))\n","\n","#6\n","#print(A/A.sum(axis=1))#A是矩阵，A.sum(axis=1)是向量，维度不同没法除，也无法广播\n","print(A/A.sum(axis=1,keepdims=True))#保持维度，不降维求和，可以通过广播跑通\n","\n","#8 0轴3x4，1轴2x4，2轴2x3\n","E=torch.arange(24,dtype=torch.float32).reshape(2,3,4)\n","print(E.sum(axis=0).shape,E.sum(axis=1).shape,E.sum(axis=2).shape)\n","\n","#9 torch.norm计算的是全部元素平方和开根\n","F=torch.ones(2,3,4)\n","G=torch.ones(2,3,4,5)\n","print(torch.norm(F)*torch.norm(F),torch.norm(G)*torch.norm(G))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[True, True, True, True, True, True, True],\n","        [True, True, True, True, True, True, True],\n","        [True, True, True, True, True, True, True]])\n","tensor([[True, True, True],\n","        [True, True, True],\n","        [True, True, True],\n","        [True, True, True],\n","        [True, True, True],\n","        [True, True, True],\n","        [True, True, True]])\n","tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])\n","6\n","tensor([[0.0000, 0.0476, 0.0952, 0.1429, 0.1905, 0.2381, 0.2857],\n","        [0.1000, 0.1143, 0.1286, 0.1429, 0.1571, 0.1714, 0.1857],\n","        [0.1176, 0.1261, 0.1345, 0.1429, 0.1513, 0.1597, 0.1681]])\n","torch.Size([3, 4]) torch.Size([2, 4]) torch.Size([2, 3])\n","tensor(24.0000) tensor(120.0000)\n"]}]},{"cell_type":"markdown","metadata":{"origin_pos":122,"tab":["pytorch"],"id":"Gotz517uuXTZ"},"source":["[Discussions](https://discuss.d2l.ai/t/1751)\n"]}]}