{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"“2.1. 数据操作代码实现.ipynb”","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_preliminaries/ndarray.ipynb","timestamp":1630754649299}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":2,"tab":["pytorch"],"id":"2zLRr2Whaq0C"},"source":["(**首先，我们导入`torch`。请注意，虽然它被称为PyTorch，但我们应该导入`torch`而不是`pytorch`。**)\n"]},{"cell_type":"code","metadata":{"origin_pos":5,"tab":["pytorch"],"id":"dC2kN1lIaq0D"},"source":["import torch\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":7,"id":"M6350DZaaq0E"},"source":["[**张量表示由一个数值组成的数组，这个数组可能有多个维度**]。具有一个轴的张量对应数学上的*向量*（vector）。具有两个轴的张量对应数学上的*矩阵*（matrix）。具有两个轴以上的张量没有特殊的数学名称。\n","\n","首先，可以使用`arange`创建一个行向量`x`。这个行向量包含从0开始的前12个整数，它们被默认创建为浮点数。张量中的每个值都称为张量的*元素*（element）。例如，张量`x`中有12个元素。除非额外指定，否则新的张量将存储在内存中，并采用基于CPU的计算。\n"]},{"cell_type":"code","metadata":{"origin_pos":9,"tab":["pytorch"],"id":"yAIv_s9maq0E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446242,"user_tz":-480,"elapsed":13,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3c93b137-4fb6-4a5f-9b2d-952b25a1d35d"},"source":["x = torch.arange(12)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"origin_pos":11,"id":"uItfL7JFaq0F"},"source":["[**可以通过张量的`shape`属性来访问张量的*形状***] (~~和张量中元素的总数~~)（沿每个轴的长度）。\n"]},{"cell_type":"code","metadata":{"origin_pos":12,"tab":["pytorch"],"id":"tm76Swebaq0F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446242,"user_tz":-480,"elapsed":10,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f9127ac0-6473-4072-8173-7b01c458009c"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"origin_pos":13,"id":"FJh8PVU3aq0G"},"source":["如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。\n","因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["pytorch"],"id":"qbP5tZMMaq0H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446242,"user_tz":-480,"elapsed":8,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f62be931-0bff-4d00-b196-763b7926ae63"},"source":["x.numel()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":17,"id":"LOM68mppaq0H"},"source":["[**要改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。**]\n","例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。注意，通过改变张量的形状，张量的大小不会改变。\n"]},{"cell_type":"code","metadata":{"origin_pos":18,"tab":["pytorch"],"id":"3LQJhYd4aq0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446243,"user_tz":-480,"elapsed":8,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3e435638-998f-4437-a4a8-ff92ff61b574"},"source":["X = x.reshape(3, 4)\n","X"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"B27h28tDaq0I"},"source":["不需要通过手动指定每个维度来改变形状。也就是说，如果我们的目标形状是（高度,宽度），那么在知道宽度后，高度应当会隐式得出，我们不必自己做除法。在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。幸运的是，张量在给出其他部分后可以自动计算出一个维度。我们可以通过在希望张量自动推断的维度放置`-1`来调用此功能。在上面的例子中，我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。\n","\n","有时，我们希望[**使用全0、全1、其他常量或者从特定分布中随机采样的数字**]来初始化矩阵。我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下：\n"]},{"cell_type":"code","metadata":{"origin_pos":22,"tab":["pytorch"],"id":"YBbTjhvTaq0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446725,"user_tz":-480,"elapsed":6,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"7e969d32-02e3-48d2-8669-1cf2996b0edf"},"source":["torch.zeros((2, 3, 4))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"ZruY-C1saq0J"},"source":["同样，我们可以创建一个形状为`(2,3,4)`的张量，其中所有元素都设置为1。代码如下：\n"]},{"cell_type":"code","metadata":{"origin_pos":26,"tab":["pytorch"],"id":"28Pq038eaq0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752446726,"user_tz":-480,"elapsed":5,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f823d477-3fcd-43b5-9eea-6706e80b5383"},"source":["torch.ones((2, 3, 4))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"-nl9P_zVaq0J"},"source":["有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。以下代码创建一个形状为（3,4）的张量。其中的每个元素都从均值为0、标准差为1的标准高斯（正态）分布中随机采样。\n"]},{"cell_type":"code","metadata":{"origin_pos":30,"tab":["pytorch"],"id":"q0ioYaSIaq0J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752447156,"user_tz":-480,"elapsed":5,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"43c0a507-47c9-40b7-b6c3-2642dc040da4"},"source":["torch.randn(3, 4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.2345,  1.2455, -0.2154, -1.1212],\n","        [-0.8914,  0.6661,  0.3034, -1.0422],\n","        [-1.8446,  0.5007, -0.5239, -0.2237]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"_3ajgh9Saq0K"},"source":["我们还可以[**通过提供包含数值的Python列表（或嵌套列表）来为所需张量中的每个元素赋予确定值**]。在这里，最外层的列表对应于轴0，内层的列表对应于轴1。\n"]},{"cell_type":"code","metadata":{"origin_pos":34,"tab":["pytorch"],"id":"WnhbyGSHaq0K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752447657,"user_tz":-480,"elapsed":6,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"defa52d2-14f2-4119-b968-ab1cc7a07c83"},"source":["torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 1, 4, 3],\n","        [1, 2, 3, 4],\n","        [4, 3, 2, 1]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"origin_pos":53,"id":"6s4vSsTLaq0M"},"source":["## 广播机制\n","\n","在某些情况下，[**即使形状不同，我们仍然可以通过调用*广播机制*（broadcasting mechanism）来执行按元素操作**]。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。\n","\n","在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\n"]},{"cell_type":"code","metadata":{"origin_pos":55,"tab":["pytorch"],"id":"vwjtEp82aq0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752453443,"user_tz":-480,"elapsed":4,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"9a3f673e-9d1a-45f3-a491-edc19a83b08c"},"source":["a = torch.arange(3).reshape((3, 1))\n","b = torch.arange(2).reshape((1, 2))\n","a, b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0],\n","         [1],\n","         [2]]), tensor([[0, 1]]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"origin_pos":57,"id":"jvVyPTb3aq0N"},"source":["由于`a`和`b`分别是$3\\times1$和$1\\times2$矩阵，如果我们让它们相加，它们的形状不匹配。我们将两个矩阵*广播*为一个更大的$3\\times2$矩阵，如下所示：矩阵`a`将复制列，矩阵`b`将复制行，然后再按元素相加。\n"]},{"cell_type":"code","metadata":{"origin_pos":58,"tab":["pytorch"],"id":"0AZVFXhLaq0N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630752459138,"user_tz":-480,"elapsed":420,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3298959b-474e-4328-efcd-e05747311b28"},"source":["a + b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [1, 2],\n","        [2, 3]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"AKB300y5kOnn"},"source":["想要广播，每个轴必须至少有一个张量是1个元素的，否则无法复制！！！"]},{"cell_type":"code","metadata":{"id":"H57JMW_jiCPN"},"source":["#C,D=torch.ones(2,3,4),torch.ones(4,3,2)#不可广播\n","C,D=torch.ones(4,1,1),torch.ones(1,3,2)#可广播\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wKB6pj4iGKb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754631759,"user_tz":-480,"elapsed":8,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"fa01db1b-84e1-4b4c-f4a2-c037b3d82bd2"},"source":["C"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1.]],\n","\n","        [[1.]],\n","\n","        [[1.]],\n","\n","        [[1.]]])"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"2V2IwHXWiHZm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754632706,"user_tz":-480,"elapsed":13,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"e1195d58-bb81-485d-fa7c-93b559043d8c"},"source":["D"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1.],\n","         [1., 1.],\n","         [1., 1.]]])"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"IgI4w8WkiHh3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754633418,"user_tz":-480,"elapsed":12,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f6cfd5f4-7b7e-4e6e-da1c-5b341073ac85"},"source":["C+D\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]]])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"jnAOsIupiHn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630754634172,"user_tz":-480,"elapsed":12,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f7ff4307-9267-45bc-ae51-6830a849a821"},"source":["print((C+D).shape,C+D)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 3, 2]) tensor([[[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.],\n","         [2., 2.]]])\n"]}]},{"cell_type":"markdown","metadata":{"origin_pos":59,"id":"3asVQh0naq0N"},"source":["## 索引和切片\n","\n","就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。与任何Python数组一样：第一个元素的索引是0；可以指定范围以包含第一个元素和最后一个之前的元素。与标准Python列表一样，我们可以通过使用负索引根据元素到列表尾部的相对位置访问元素。\n","\n","因此，我们[**可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素**]，如下所示：\n"]},{"cell_type":"code","metadata":{"id":"Wh2JTXXuiOis"},"source":["X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uxh_2nILiTnO"},"source":["X[-1], X[1:3]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":61,"tab":["pytorch"],"id":"XtGzVsSTaq0O"},"source":["[**除读取外，我们还可以通过指定索引来将元素写入矩阵。**]\n"]},{"cell_type":"code","metadata":{"id":"k6bqayX_iVN2"},"source":["X[1, 2] = 9\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":65,"id":"qAQC1FInaq0O"},"source":["如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**]\n","例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。\n"]},{"cell_type":"code","metadata":{"id":"YBcAdSGViWeX"},"source":["X[0:2, :] = 12\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":68,"id":"5oTFM5ldaq0O"},"source":["## 节省内存\n","\n","[**运行一些操作可能会导致为新结果分配内存**]。例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。\n","\n","在下面的例子中，我们用Python的`id()`函数演示了这一点，它给我们提供了内存中引用对象的确切地址。运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。\n"]},{"cell_type":"code","metadata":{"origin_pos":69,"tab":["pytorch"],"id":"Wd6E36Praq0P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630753748485,"user_tz":-480,"elapsed":15,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f6420c1e-b2e1-4019-ea2f-7a461e515604"},"source":["before = id(Y)\n","Y = Y + X\n","id(Y) == before"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"origin_pos":70,"id":"jpXGyFeRaq0P"},"source":["这可能是不可取的，原因有两个：首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新。其次，我们可能通过多个变量指向相同参数。如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n"]},{"cell_type":"markdown","metadata":{"origin_pos":71,"tab":["pytorch"],"id":"1i4p0DTuaq0P"},"source":["幸运的是，(**执行原地操作**)非常简单。我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同，使用`zeros_like`来分配一个全$0$的块。\n"]},{"cell_type":"code","metadata":{"origin_pos":74,"tab":["pytorch"],"id":"4yuDn1Wvaq0P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630753698549,"user_tz":-480,"elapsed":937,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"62bbf229-db53-41e0-d515-a48684683c26"},"source":["Z = torch.zeros_like(Y)\n","Z"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"OjRLUWc1frNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630753707350,"user_tz":-480,"elapsed":1024,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"82572037-28dd-430e-f957-18adc52f4cb4"},"source":["print('id(Z):', id(Z))\n","Z[:] = X + Y\n","print('id(Z):', id(Z))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id(Z): 139775823837536\n","id(Z): 139775823837536\n"]}]},{"cell_type":"markdown","metadata":{"origin_pos":76,"tab":["pytorch"],"id":"0T4e4fY-aq0P"},"source":["[**如果在后续计算中没有重复使用`X`，我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。**]\n"]},{"cell_type":"code","metadata":{"origin_pos":78,"tab":["pytorch"],"id":"Zk16be-Oaq0P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630753844070,"user_tz":-480,"elapsed":1143,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1e5cb648-7fc6-47fe-8a38-2266143e1773"},"source":["before = id(X)\n","X += Y\n","id(X) == before"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"origin_pos":80,"id":"bm7xqmwSaq0Q"},"source":["## 转换为其他 Python 对象\n","\n","[**转换为NumPy张量**]很容易，反之也很容易。转换后的结果不共享内存。\n","这个小的不便实际上是非常重要的：当你在CPU或GPU上执行操作的时候，如果Python的NumPy包也希望使用相同的内存块执行其他操作，你不希望停下计算来等它。\n"]},{"cell_type":"code","metadata":{"origin_pos":82,"tab":["pytorch"],"id":"IV4YGm08aq0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630753889119,"user_tz":-480,"elapsed":1351,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"013539d1-d0d9-48a1-96f6-bfada61a05e7"},"source":["A = X.numpy()\n","B = torch.tensor(A)\n","type(A), type(B)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(numpy.ndarray, torch.Tensor)"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"origin_pos":84,"id":"OzCJSEipaq0Q"},"source":["要(**将大小为1的张量转换为Python标量**)，我们可以调用`item`函数或Python的内置函数。\n"]},{"cell_type":"code","metadata":{"origin_pos":86,"tab":["pytorch"],"id":"ivy6L7aZaq0Q","outputId":"f6d3df23-4342-4ded-b017-11d88f10fd2d"},"source":["a = torch.tensor([3.5])\n","a, a.item(), float(a), int(a)"],"execution_count":null,"outputs":[{"data":{"text/plain":["(tensor([3.5000]), 3.5, 3.5, 3)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"IStB9XEkkUFJ"},"source":["https://zhuanlan.zhihu.com/p/377032909课后练习代码讲解"]}]}